# Positronic Imitation Learning Workflow

This document walks you through collecting demonstrations, preparing datasets, training an imitation-learning policy, and validating the result inside the Positronic project. Each stage points to the script or configuration you actually run so you can reach a working policy with minimal guesswork.

---

## 1. Environment Setup

Positronic relies on [uv](https://docs.astral.sh/uv/) for locked, reproducible environments. Start from a freshly cloned repository:

```bash
uv venv -p 3.11               # optional but keeps the project isolated
source .venv/bin/activate     # only if you created the venv above
uv sync --frozen --extra dev  # install everything required for development & docs
```

Install hardware support only if you plan to drive a physical robot:

```bash
uv sync --frozen --extra hardware
```

All runtime commands below work either inside the activated virtual environment or via `uv run --with-editable . -s …`.

---

## 2. Workflow at a Glance

1. **Record demonstrations** with `python -m positronic.data_collection …` (simulation or hardware teleoperation).
2. **Review and triage recordings** using the browser viewer in `python -m positronic.server.positronic_server`.
3. **Convert to LeRobot format** through `positronic/training/to_lerobot.py`. This is a temporary step; direct training on the native Positronic dataset is coming soon.
4. **Train a policy** via `positronic/training/lerobot_train.py`.
5. **Validate the policy** in simulation using `python -m positronic.run_inference …` (optionally logging fresh evaluation episodes).

Every command writes outputs to disk so you can rerun or branch without losing previous work.

---

## 3. Collect Demonstrations (`positronic/data_collection.py`)

### 3.1 Quick start in simulation

```bash
python -m positronic.data_collection sim \
    --output_dir=~/datasets/stack_cubes_raw \
    --webxr=.oculus \
    --operator_position=.BACK
```

This launches the MuJoCo scene (`positronic/assets/mujoco/franka_table.xml` with loaders from `positronic/cfg/simulator.py`), spawns the control UI from `positronic/gui/dpg.py`, and records into a `LocalDatasetWriter` (`positronic/dataset/local_dataset.py`).

### 3.2 Driving physical robots

Pick the preset closest to your hardware — each configuration is defined in `positronic/data_collection.py` using `configuronic`:

```bash
python -m positronic.data_collection real  --output_dir=~/datasets/franka_kitchen
python -m positronic.data_collection so101 --output_dir=~/datasets/so101_runs
python -m positronic.data_collection droid --output_dir=~/datasets/droid_runs
```

Override components inline (for example `--webxr=.iphone`, `--sound=None`, `--operator_position=.FRONT`) or create your own configs under `positronic/cfg/hardware/` and reuse them here.

### 3.3 Using a VR headset (Meta Quest / Oculus)

1. Run one of the commands above with `--webxr=.oculus`.
2. Find the host machine’s IP address (e.g. `ipconfig getifaddr en0` on macOS, `hostname -I` on Linux) and keep the headset on the same Wi‑Fi.
3. In the headset open **Oculus Browser**, type `https://<host-ip>:5005/`, and confirm the security warning once (“Advanced” → “Continue” – it is the self-signed cert generated by `positronic/drivers/webxr.py`).
4. When the page loads, press the **Enter AR** button that appears in the page header. Approve the permission dialog so the browser can access tracking data.
5. You now see floating controller axes and, if you started the collector with `--stream_video_to_webxr=<camera_name>`, a live panel showing that camera feed inside the headset. The `<camera_name>` must match a key from the `cameras` dictionary wired in `positronic/data_collection.py`.
6. Control mappings (sent from `positronic/utils/buttons.py`):
   - **Right B** — start/stop recording (you’ll hear the audio cues if sound is enabled).
   - **Right A** — toggle positional tracking so you can reposition the robot with the controller pose.
   - **Right stick press** — abort the current episode and invoke `roboarm.command.Reset()`.
   - **Right trigger** — analogue gripper command (also recorded into the dataset).

### 3.4 Using an iPhone as a controller

1. Launch data collection with `--webxr=.iphone` (note the frontend in `positronic/cfg/webxr.py`).
2. On the phone open **XR Browser** (or another WebXR-capable browser) and navigate to `http://<host-ip>:5005/` — the iPhone frontend uses HTTP for a frictionless connection.
3. Tap **Enter AR**, accept the camera/gyroscope permission, and point the phone upright; the 3D reticle represents the virtual controller.
4. Use the on-screen controls provided by `positronic/assets/webxr_iphone/index.html`:
   - **Record** button toggles episode recording.
   - **Track** button toggles tracking.
   - **Reset** button aborts the current run and resets the robot.
   - **Gripper slider** drives the analogue trigger, with padding for easier access to fully open/close.

### 3.5 Tips for consistent demonstrations

- Keep an eye on the DearPyGui window to ensure cameras and robot state update smoothly; if a stream freezes, restart the episode rather than saving partial data.
- Capture short calibration clips first, then inspect them in the viewer (next section) before investing time into a full dataset.
- Use episode static metadata (e.g. task name) to encode the intention behind each run; you can populate it in the stop dialog that appears in the GUI.

---

## 4. Review Datasets (`positronic/server/positronic_server.py`)

Launch the viewer once you have a few episodes:

```bash
python -m positronic.server.positronic_server \
    --root=~/datasets/stack_cubes_raw \
    --port=5001 \
    --reset_cache=True
```

What you get:

- **Episode list** — the index page shows the episode count, durations, and stored `task` metadata (provided by `positronic/server/dataset_utils.py`).
- **Rerun timeline** — opening `/episode/<id>` streams RGB feeds, plots numeric signals, and overlays the task text so you can scrub through a demonstration.
- **Caching** — generated `.rrd` bundles live under `~/.cache/positronic/server/`, so subsequent loads are instant.

The server is read-only at the moment: though we plan to add capabilities to edit episodes. You can note poor-quality runs while watching and remove the corresponding directories manually.

---

## 5. Convert to LeRobot (`positronic/training/to_lerobot.py`)

Until the training scripts consume Positronic datasets directly, convert each curated dataset into LeRobot format:

```bash
uv run --with-editable . -s positronic/training/to_lerobot.py convert \
    --dataset.path=~/datasets/stack_cubes_raw \
    --output_dir=~/datasets/lerobot/stack_cubes \
    --task="pick up the green cube and place it on the red cube" \
    --fps=30
```

Key facts:

- The converter loads your dataset through `positronic.cfg.dataset.transformed`, which applies the same observation/action transforms used at inference time.
- Each Positronic episode becomes a LeRobot episode while preserving metadata and videos.
- Re-run the converter whenever you add new demos or change transforms; the process is deterministic.

Appending new demonstrations later is symmetric:

```bash
uv run --with-editable . -s positronic/training/to_lerobot.py append \
    --lerobot_dataset_dir=~/datasets/lerobot/stack_cubes \
    --dataset.path=~/datasets/stack_cubes_new
```

Keep the original Positronic recordings — once direct training support lands you will be able to skip this conversion step.

---

## 6. Train a Policy (`positronic/training/lerobot_train.py`)

Run LeRobot’s training pipeline with Positronic defaults:

```bash
uv run --with-editable . -s positronic/training/lerobot_train.py \
    --dataset_root=~/datasets/lerobot/stack_cubes \
    --base_config=positronic/training/train_config.json
```

The script kicks off ACT training, and writes trained checkpoints under `outputs/train/<timestamp>_<job_name>/`.
Adjust the script if you want different hyperparameters, vision backbones, or devices.

---

## 7. Validate Policies (`positronic/run_inference.py`)

Check the trained policy in simulation:

```bash
python -m positronic.run_inference sim_act \
    --policy.checkpoint_path=~/datasets/lerobot/stack_cubes/checkpoints/last/pretrained_model \
    --device=mps \
    --simulation_time=60 \
    --output_dir=~/datasets/inference_logs/stack_cubes_act \
    --show_gui
```

`positronic/run_inference.py` reuses the MuJoCo scene, decodes camera frames, and turns network actions into `roboarm.command.CartesianMove` commands. Provide `--output_dir` when you want to log the rollout; the script dumps into Positronic Dataset format, so you can look up what's going on with [Positronic Server](positronic/server/positronic_server.py).
